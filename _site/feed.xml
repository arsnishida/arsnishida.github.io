<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-01T11:52:22-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ARSN Lab Notebook</title><subtitle>ARSN Lab notebook.</subtitle><entry><title type="html">OneDrive Upload and Download with Rclone</title><link href="http://localhost:4000/2022/03/01/post-0074.html" rel="alternate" type="text/html" title="OneDrive Upload and Download with Rclone" /><published>2022-03-01T00:00:00-08:00</published><updated>2022-03-01T00:00:00-08:00</updated><id>http://localhost:4000/2022/03/01/post-0074</id><content type="html" xml:base="http://localhost:4000/2022/03/01/post-0074.html">&lt;p&gt;Here’s instructions for using rclone on our clusters specifically using a remote/headless machine. This requires more setup but skips the use of X11.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download rclone on your personal home computer, https://rclone.org/downloads/.&lt;/li&gt;
  &lt;li&gt;Go to mays/mccovey/bonds and load the rclone modules, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module load rclone&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Setup rclone on mays/mccovey/bonds, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rclone config&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Inside rclone’s configuration, setup the following:
    &lt;ul&gt;
      &lt;li&gt;‘n’ for new remote at the ‘n/s/q’ prompt&lt;/li&gt;
      &lt;li&gt;a shortcut name for the storage at the ‘name’ prompt, I am just calling it ONEDRIVE&lt;/li&gt;
      &lt;li&gt;‘onedrive’ for the ‘Storage’ prompt&lt;/li&gt;
      &lt;li&gt;press enter to skip the prompt ‘client_id’&lt;/li&gt;
      &lt;li&gt;press enter to skip the prompt ‘client_secret’&lt;/li&gt;
      &lt;li&gt;‘global’ for the prompt ‘region’&lt;/li&gt;
      &lt;li&gt;‘n’ to not edit the advanced configurations at ‘y/n’&lt;/li&gt;
      &lt;li&gt;‘n’ to setup on the headless node at ‘y/n’&lt;/li&gt;
      &lt;li&gt;leave this open for a moment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;On your personal home computer run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rclone authorize &quot;onedrive&quot;&lt;/code&gt;. This will open your web browser and ask you to log into your OneDrive account. This then returns a long configuration string to copy.&lt;/li&gt;
  &lt;li&gt;Paste the configuration key back on the config in mays/mccovey/bonds.&lt;/li&gt;
  &lt;li&gt;Finish configuration:
    &lt;ul&gt;
      &lt;li&gt;confirm the configuration as ‘onedrive’ at the prompt ‘config_type’&lt;/li&gt;
      &lt;li&gt;confirm ‘y’ at the URL confirmation of ‘y/n’&lt;/li&gt;
      &lt;li&gt;confirm ‘y’ for setup at the last prompt ‘y/n’&lt;/li&gt;
      &lt;li&gt;‘q’ to quit out and you should be set up&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Send a test file. This should send the test file to your OneDrive account setup with the name “ONEDRIVE” to the directory Documents/
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo &quot;THIS IS A TEST FILE&quot; &amp;gt; test.txt
rclone copy test.txt ONEDRIVE:Documents/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="ref" /><summary type="html">Here’s instructions for using rclone on our clusters specifically using a remote/headless machine. This requires more setup but skips the use of X11.</summary></entry><entry><title type="html">Dropbox Upload and Download with Curl and API Key</title><link href="http://localhost:4000/2022/01/25/post-0073.html" rel="alternate" type="text/html" title="Dropbox Upload and Download with Curl and API Key" /><published>2022-01-25T00:00:00-08:00</published><updated>2022-01-25T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/25/post-0073</id><content type="html" xml:base="http://localhost:4000/2022/01/25/post-0073.html">&lt;p&gt;These are the step to upload and download from the console to and from Dropbox.
&lt;br /&gt;1. Go to: https://www.dropbox.com/developers/apps
&lt;br /&gt;2. Create an App. Name is whatever you like but probably something like “Dropbox Access”.
&lt;br /&gt;3. Navigate to Permissions on the top tab. Check account_info.write, account_info.read, files.metadata.write, files.metadata.read, files.content.write, and files.content.read.
&lt;br /&gt;4. Navigate to Settings on the top tab. Generate your access token. If you change your permissions you must regenerate your access token. Always keep your API key secret or else anybody can use it to access everything in your Dropbox.&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;&lt;b&gt;For files &amp;lt; 150 Mb using the “2/files/upload” endpoint.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Upload Instructions. This will upload file, ‘test.txt’, into your Dropbox folder ‘temp_upload’ under the name ‘uploaded_test_file.txt’.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# create test file
echo &quot;TEST THIS&quot; &amp;gt; test.txt

# fill these variables in
API_KEY=&quot;keep your API key secret&quot;
INPUT_PATH=&quot;test.txt&quot;
DROPBOX_FILE_PATH=&quot;/temp_upload/uploaded_test_file.txt&quot;

# run
curl -X POST https://content.dropboxapi.com/2/files/upload \
    --header &quot;Authorization: Bearer ${API_KEY}&quot; \
    --header &quot;Dropbox-API-Arg: {\&quot;path\&quot;: \&quot;${DROPBOX_FILE_PATH}\&quot;}&quot; \
    --header &quot;Content-Type: application/octet-stream&quot; \
    --data-binary @${INPUT_PATH}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Download Instructions. This will download ‘uploaded_test_file.txt’ in Dropbox directory ‘temp_upload’ as ‘downloaded_test_file.txt&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# fill these variables in
API_KEY=&quot;keep your API key secret&quot;
DROPBOX_FILE_PATH=&quot;/temp_upload/uploaded_test_file.txt&quot;
OUTPUT_PATH=&quot;downloaded_test_file.txt&quot;

# run
curl -X POST https://content.dropboxapi.com/2/files/download \
    --header &quot;Authorization: Bearer ${API_KEY}&quot; \
    --header &quot;Dropbox-API-Arg: {\&quot;path\&quot;: \&quot;${DROPBOX_FILE_PATH}\&quot;}&quot; \
    -o &quot;${OUTPUT_PATH}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;&lt;b&gt;For files &amp;gt; 150 Mb using upload sessions.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;https://github.com/andreafabrizi/Dropbox-Uploader&lt;/p&gt;</content><author><name></name></author><category term="ref" /><summary type="html">These are the step to upload and download from the console to and from Dropbox. 1. Go to: https://www.dropbox.com/developers/apps 2. Create an App. Name is whatever you like but probably something like “Dropbox Access”. 3. Navigate to Permissions on the top tab. Check account_info.write, account_info.read, files.metadata.write, files.metadata.read, files.content.write, and files.content.read. 4. Navigate to Settings on the top tab. Generate your access token. If you change your permissions you must regenerate your access token. Always keep your API key secret or else anybody can use it to access everything in your Dropbox.</summary></entry><entry><title type="html">Drosophilia Tau DA Results Test Analysis</title><link href="http://localhost:4000/2022/01/21/post-0072.html" rel="alternate" type="text/html" title="Drosophilia Tau DA Results Test Analysis" /><published>2022-01-21T00:00:00-08:00</published><updated>2022-01-21T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/21/post-0072</id><content type="html" xml:base="http://localhost:4000/2022/01/21/post-0072.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis&lt;/code&gt;
&lt;br /&gt;Run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is an example rough pass through analyzing the results of the DA test. This focuses on the ‘3_Parent_d5’ comparisons as the example. Clusters are combined across treatments but kept separate. Thresholds for ‘significantly changing’ sites are &amp;lt; 0.01 raw p-value and &amp;gt; 1.5 Log2 FC. The significant results could be annotated by their motifs and nearest genes.&lt;/p&gt;

&lt;p&gt;5.2-5.7 - P-value distributions are left-skewed. FDR does not work except maybe with WT_to_KI_KI. Volcano plots suggest global opening and closing in some conditions (WT_to_KI_KI shows a global decrease in accessibility across many sites).
8.8 - Heatmap shows groups of sites that are specific to the strains trending in the same direction. The most notable is the group for WT to KI_KI which shows large decreases specific to the biological cluster 3 (in yellow on the left).&lt;/p&gt;

&lt;p&gt;Markdown links:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/2vbwjy5j27zohi0/dm6_test_da.html?dl=0&quot;&gt;R Markdown HTML&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/an2v1ngucr8nqlt/dm6_test_da.Rmd?dl=0&quot;&gt;R Markdown&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="dm6_tau" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis Run: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis/run.txt</summary></entry><entry><title type="html">SPARK Pre Best Practices Filter, Model Scores</title><link href="http://localhost:4000/2022/01/19/post-0071.html" rel="alternate" type="text/html" title="SPARK Pre Best Practices Filter, Model Scores" /><published>2022-01-19T00:00:00-08:00</published><updated>2022-01-19T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/19/post-0071</id><content type="html" xml:base="http://localhost:4000/2022/01/19/post-0071.html">&lt;p&gt;This markdown takes the pre-filtered data and plots out the metrics that are used for scoring by the logistic model. All variants are plotted and then just the subset of mosaic child calls.&lt;/p&gt;

&lt;p&gt;This data represents the simplex quads only. It uses the broad filter and removes parental germline calls.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A broad filter to reduce computational load is used.
- Frequent variants appearing 5 or more times are removed from further analysis.
- Common variant allele frequencies determined from ExAC and gnomAD annotations must be less than 0.005.
- Variants require at least 3 non-reference reads and at least 8 reads in all four family members.
- Variants must also fall within exonic or splicing regions from RefSeq annotations.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Markdown showing the error model metrics of the pre-filtered data:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/md88g4ahmmwuer9/20_logistic_model_params.html?dl=0&quot;&gt;20 Logistic Model Metrics Pre-Filter&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">This markdown takes the pre-filtered data and plots out the metrics that are used for scoring by the logistic model. All variants are plotted and then just the subset of mosaic child calls.</summary></entry><entry><title type="html">SPARK SSC hg38 Transformed Burden, 01</title><link href="http://localhost:4000/2022/01/18/post-0070.html" rel="alternate" type="text/html" title="SPARK SSC hg38 Transformed Burden, 01" /><published>2022-01-18T00:00:00-08:00</published><updated>2022-01-18T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/18/post-0070</id><content type="html" xml:base="http://localhost:4000/2022/01/18/post-0070.html">&lt;p&gt;The SSC list was transformed from hg19 to hg38 as described &lt;a href=&quot;https://arsnishida.github.io/2021/11/30/post-0066.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;20 sites were removed for having no original FRACTION assignment. 172 sites removed for falling into a SDTRF region in going from hg19 to hg38. 9 sites were removed for having no functional mutation type (both originally and after reannotation). There were 7 mosaic child sites lost in this fashion. 1 had no mutation type in both hg19 and hg38. The other 6 fell in super duplicate regions with 3 of them being within 100 bp of each other constituting a complex event.&lt;/p&gt;

&lt;p&gt;This recreates the burden analysis from the original SSC paper. The trends match but the n’s and signficance do not. This is due to the family list and potentially the LGD/NS lists. The family lists include hundreds more families than the original analysis and are taken from ‘cohort_all_families_list.table’. The LGD/NS lists were taken from ‘Family_Subcohort_Designations.xlsx’. These were found in our Box directory:
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/All Files/kruppd/Writing/SSC somatic pilot/data/Data Used for Mosaic Paper/High Confidence Variants/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Markdown showing the SSC solo burden analysis:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/0pv9gk7cxk24v9j/19_burden_genesets_ssc_missense.html?dl=0&quot;&gt;19 SSC Solo Burden Markdown&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">The SSC list was transformed from hg19 to hg38 as described here.</summary></entry><entry><title type="html">Drosophilia Tau Final DA</title><link href="http://localhost:4000/2022/01/08/post-0069.html" rel="alternate" type="text/html" title="Drosophilia Tau Final DA" /><published>2022-01-08T00:00:00-08:00</published><updated>2022-01-08T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/08/post-0069</id><content type="html" xml:base="http://localhost:4000/2022/01/08/post-0069.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022&lt;/code&gt;
&lt;br /&gt;Run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The DA was performed after TF normalization using the Wilcoxon rank sum test and creates the comparisons in ‘210712_DA_Comparisons.txt’. Mild name changes were made in the comparisons such as ‘Photoreceptor’ to ‘Photoreceptors’ to match the annotation file and using the correct annotation column for the biologically split subsets.&lt;/p&gt;

&lt;p&gt;The following comparisons are nonexistant because they have no cells with the annotation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Glia_WT_CS_5_C13 has no cells so all 6 comparisons are missing in 3_Glia_d5.
- Glia_KI_CS_30_C21, Glia_dTaudel_30_C13, Glia_KI_CS_30_C13 in 3_Glia_d30.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following comparisons are deliberately empty because they have too few cells:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- DA.Glia_WT_CS_30_C11_Glia_PL_CS_30_C11.txt, DA.Glia_WT_CS_30_C13_Glia_CS_30_C13.txt
- DA.Neuron_WT_CS_30_C14_Neuron_dTaudel_30_C14.txt, DA.Neuron_WT_CS_30_C14_Neuron_PL_CS_30_C14.txt
- DA.Photoreceptors_WT_CS_30_C1_Photoreceptors_PL_CS_30_C1.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_dTaudel_30_C3.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_KI_CS_30_C3.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_PL_CS_30_C3.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_VM_CS_30_C3.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="dm6_tau" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022 Run: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/run.txt</summary></entry><entry><title type="html">SPARK VEP Annotations</title><link href="http://localhost:4000/2021/12/03/post-0068.html" rel="alternate" type="text/html" title="SPARK VEP Annotations" /><published>2021-12-03T00:00:00-08:00</published><updated>2021-12-03T00:00:00-08:00</updated><id>http://localhost:4000/2021/12/03/post-0068</id><content type="html" xml:base="http://localhost:4000/2021/12/03/post-0068.html">&lt;p&gt;VEP was used to annotate the high confidence lists. The following databases were used. There are redundant scores between the databases but they might possibly differ slightly due to the different versions of references each uses.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;dbNSFP v4.1a from ftp://dbnsfp:dbnsfp@dbnsfp.softgenetics.com/dbNSFP4.1a.zip&lt;/li&gt;
  &lt;li&gt;gnomAD v3.0.1 from https://storage.googleapis.com/gcp-public-data–gnomad/release/3.0.1/coverage/genomes/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz&lt;/li&gt;
  &lt;li&gt;REVEL v1.3 from https://sites.google.com/site/revelgenomics/downloads&lt;/li&gt;
  &lt;li&gt;LoFtool scores (no version) from https://github.com/Ensembl/VEP_plugins&lt;/li&gt;
  &lt;li&gt;ExACpLI scores (no version) from https://github.com/Ensembl/VEP_plugins&lt;/li&gt;
  &lt;li&gt;blosum62 scores (no version)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am keeping this separated from the high confidence lists but these columns can be directly added to the highconfidence lists without any reordering (the rows are the same). These are available here. 
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/bjwj52ibth2rytw/quadsimplex.highconfidence.add_annot.txt?dl=0&quot;&gt;Simplex Quad, Added Annotations&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/fgr8kt3mt98juq5/quadmultiplex.highconfidence.add_annot.txt?dl=0&quot;&gt;Multiplex Quad, Added Annotations&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/ntn4cr1jmh44q70/triosimplex.highconfidence.add_annot.txt?dl=0&quot;&gt;Simplex Trio, Added Annotations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The dbNSFP columns are described here and then are followed by the other scores from the other annotation databases:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/j95lkv05al56kcp/dbNSFP_columns.txt?dl=0&quot;&gt;Added Annotation Column Descriptions&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">VEP was used to annotate the high confidence lists. The following databases were used. There are redundant scores between the databases but they might possibly differ slightly due to the different versions of references each uses. dbNSFP v4.1a from ftp://dbnsfp:dbnsfp@dbnsfp.softgenetics.com/dbNSFP4.1a.zip gnomAD v3.0.1 from https://storage.googleapis.com/gcp-public-data–gnomad/release/3.0.1/coverage/genomes/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz REVEL v1.3 from https://sites.google.com/site/revelgenomics/downloads LoFtool scores (no version) from https://github.com/Ensembl/VEP_plugins ExACpLI scores (no version) from https://github.com/Ensembl/VEP_plugins blosum62 scores (no version)</summary></entry><entry><title type="html">SPARK Burden Tables Gene Sets, SPARK+SSC</title><link href="http://localhost:4000/2021/11/30/post-0067.html" rel="alternate" type="text/html" title="SPARK Burden Tables Gene Sets, SPARK+SSC" /><published>2021-11-30T00:00:00-08:00</published><updated>2021-11-30T00:00:00-08:00</updated><id>http://localhost:4000/2021/11/30/post-0067</id><content type="html" xml:base="http://localhost:4000/2021/11/30/post-0067.html">&lt;p&gt;This performs the burden analysis across the genesets using the combined and harmonized Spark and SSC lists. The SSC is transformed to hg38 but the joint coverages remain in hg19. Joint coverages were manually filtered for duplicate entries for two families, 13431 and 12839.&lt;/p&gt;

&lt;p&gt;Markdowns:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/9j19yvd2bc66lyi/18_burden_genesets_qsts_ssc_missense.html?dl=0&quot;&gt;18 Burden Genesets, SPARK+SSC and missense&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/nh5cqxu9qosk6aj/18_burden_genesets_qsts_ssc_synonymous.html?dl=0&quot;&gt;18 Burden Genesets, SPARK+SSC and synonymous&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/lu5b5ruetw526jr/18_burden_genesets_qsts_ssc_nonsensesplicing.html?dl=0&quot;&gt;18 Burden Genesets, SPARK+SSC and nonsense-splicing&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">This performs the burden analysis across the genesets using the combined and harmonized Spark and SSC lists. The SSC is transformed to hg38 but the joint coverages remain in hg19. Joint coverages were manually filtered for duplicate entries for two families, 13431 and 12839.</summary></entry><entry><title type="html">SPARK SSC List Formatting to Current SPARK</title><link href="http://localhost:4000/2021/11/30/post-0066.html" rel="alternate" type="text/html" title="SPARK SSC List Formatting to Current SPARK" /><published>2021-11-30T00:00:00-08:00</published><updated>2021-11-30T00:00:00-08:00</updated><id>http://localhost:4000/2021/11/30/post-0066</id><content type="html" xml:base="http://localhost:4000/2021/11/30/post-0066.html">&lt;p&gt;The SSC variant calls are taken from this Box file, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://ohsu.box.com/s/dqm3garunt0xobvgdz41onpexi87yqin&lt;/code&gt;, on this path, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;All Files/kruppd/Writing/SSC somatic pilot/data/Data Used for Mosaic Paper/High Confidence Variants/HighConfidence Variants Collapsed_2.csv&lt;/code&gt;. This appears to be a similar version of Table S5 from the Krupp 2017 paper with the columns with the statistics which are excluded from the paper. Unlike the table in the paper, this also collapses sites in overlapping genes into a single entry which is more in line with the SPARK lists where genes are combined in one entry separated by commas. All SSC columns are transformed to match the columns for the SPARK list.&lt;/p&gt;

&lt;p&gt;Here is the transformed SSC list. It has the same columns as the SPARK lists and has extra columns representing the SSC specific entries. The two important extra columns to consider that are unique to the SSC list are column 90 and 91 which are the ‘FILTER’ and ‘OLD_TYPE’ respectively. ‘FILTER’ is TRUE or FALSE. TRUE entries should be filtered from the analysis and their designation is explained below. In general they are calls that do not meet our criteria after reannotation in hg38 versus hg19. ‘OLD_TYPE’ designates whether the call comes from a quad or trio family which may or may not be important for us.&lt;/p&gt;

&lt;p&gt;SSC list in SPARK list format:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/hkjkeqibfk2nkyw/ssc.transformed.txt?dl=0&quot;&gt;SSC List in SPARK List Format&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Markdown showing how the list was modified:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/j1flcekwsrp6s03/17_ssc_list.html?dl=0&quot;&gt;17 SSC List Formatting to Current SPARK&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The column ‘FILTER’ designates calls to remove from downstream analysis. ‘TRUE’ calls should be removed and are given if:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- there are no FRACTION calls made (not labeled GERMLINE or MOSAIC, labeled 'NA')
- the variant falls in a region that is annotated in either a non-exonic, super duplicate, or trf region
- the variant does not have a predicted functional mutation type
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following columns were modified in the transformation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Coordinates were changed via LiftOver with 100% of the sites found going from hg19 to hg38.
All genomic and functional annotations were redone with AnnoVAR for hg38.
FRACTION_MORE is derived from FRACTION with name changes to match the SPARK list (&quot;GERMLINE_C&quot; is changed to &quot;GERMLINE_CHILD&quot;).
ACC_DIST and DON_DIST are the same value. The SSC list keeps only the closest wheras the SPARK list keeps both separate.
COMPLEX_EVENT is descriptive in the SSC list and binary in the SPARK list.
NUM_FAMILY_FOUND and filteredCohortCount represent cohort counts before and after filtering. In the SSC list, only the final count is kept so these are the same for the SSC list.
HGNCsymbol_to_ENSID and RefSeq_to_ENSID are the ENSEMBL ID's determined from the hg38 annotations.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following columns were dropped and all set to ‘NA’:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;There are no numerical sample IDs in the SSC list.
SSC uses combined family stats from the variant callers whereas the SPARK list keeps them by individual so these are dropped.
Mismatch stats for raw mismatch counts, 'other' read counts, and the means are not present in the SSC list.
gnomAD allele fractions are not present in the earlier SSC table.
The only error columns in the SSC list are the calculated rates.
FRACTION_OLD is a legacy column in the SPARK list and does not have meaning in the SSC list.
The splice site transcript IDs are not present just the distances.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following extra columns specific to the SSC list were added at the end. These are potentially important for analysis:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FILTER - TRUE or FALSE and represent variants that should be filtered out after the transformation of hg19 to hg38
OLD_TYPE - TRIO or QUAD
OLD_SEGDUP, OLD_RMSK, OLD_TRF, OLD_FUNC_TYPE, OLD_TRANS_REF, OLD_CLASS_REF, OLD_GENE_REF - these are the original hg19 AnnoVAR annotations
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following SSC specific columns were kept and added at the end. These can likely be ignored. I’ve tried to describe them as best as possible without knowing their exact provenance:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OLD_STUDY - COHORT or PILOT24 or PILOT400
OLD_PUBLISHED - IOSSIFOV_2014 or KRUMM_2015
OLD_AF_ESP, OLD_AF_1KG - allele fractions from other databases
OLD_PCTILE_CDS_AUTO_45X - 5% percentiles of CDS AUTO 45x
OLD_FORMAT_VARSCAN, OLD_FORMAT_LOFREQ, OLD_FORMAT_SMOC, OLD_VARSCAN_232_FA, OLD_LOFREQ_211_FA, OLD_SMOC_FA, OLD_VARSCAN_232_MO, OLD_LOFREQ_211_MO, OLD_SMOC_MO, OLD_VARSCAN_232_PRO, OLD_LOFREQ_211_PRO, OLD_SMOC_PRO, OLD_VARSCAN_232_SIB, OLD_LOFREQ_211_SIB, OLD_SMOC_SIB - family-based variant calling output
OLD_DIST_TO_PREV, OLD_DIST_TO_NEXT, OLD_PERSON_PREV, OLD_PERSON_NEXT - closest call and person	
OLD_QC_STATUS - at this stage they are all 'PASS', unknown criteria
OLD_COHORT_DP, OLD_COHORT_DPALT, OLD_P400_COUNT_MOS, OLD_P400_COUNT_GERM, OLD_COHORT_COUNT_MOS, OLD_COHORT_COUNT_GERM, OLD_COHORT_MAX_AD, OLD_P400_COUNT_ALL - various counts within the cohort subsets
OLD_AF_FA, OLD_AF_MO, OLD_AF_PRO, OLD_AF_SIB, OLD_CALLED_FA, OLD_CALLED_MO, OLD_CALLED_PRO, OLD_CALLED_SIB, OLD_DETECT_FA, OLD_DETECT_MO, OLD_DETECT_PRO, OLD_DETECT_SIB, OLD_PHET_MAIN, OLD_SITE_KEY, OLD_PARENTAL, OLD_GONADAL, OLD_CALLED_VARSCAN, OLD_CALLED_LOFREQ, OLD_CALLED_SMOC - these are all redundant derived columns
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">The SSC variant calls are taken from this Box file, https://ohsu.box.com/s/dqm3garunt0xobvgdz41onpexi87yqin, on this path, All Files/kruppd/Writing/SSC somatic pilot/data/Data Used for Mosaic Paper/High Confidence Variants/HighConfidence Variants Collapsed_2.csv. This appears to be a similar version of Table S5 from the Krupp 2017 paper with the columns with the statistics which are excluded from the paper. Unlike the table in the paper, this also collapses sites in overlapping genes into a single entry which is more in line with the SPARK lists where genes are combined in one entry separated by commas. All SSC columns are transformed to match the columns for the SPARK list.</summary></entry><entry><title type="html">Single Cell TBR1 Pseudo-Bulk Depth Tests</title><link href="http://localhost:4000/2021/11/28/post-0065.html" rel="alternate" type="text/html" title="Single Cell TBR1 Pseudo-Bulk Depth Tests" /><published>2021-11-28T00:00:00-08:00</published><updated>2021-11-28T00:00:00-08:00</updated><id>http://localhost:4000/2021/11/28/post-0065</id><content type="html" xml:base="http://localhost:4000/2021/11/28/post-0065.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_tmp1/pseudo_bulk_example&lt;/code&gt;
&lt;br /&gt;Run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_tmp1/pseudo_bulk_example/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This gives an example of how to take the single-cell data and transform it to bulk. I then subsampled the full BAM by increments of 5% and called macs3 on each subset. The plot shows a linear increase of the number of peaks called relative to the log number of reads present. This indicates that we are not at saturation of reads for calling peaks and gives a rough approximation of what we can expect. From my personal experience with bulk DNase data, saturation for peaks called needs ~500M reads. With single-cell data you get a lot more repetition of common open sites from each individual cell.&lt;/p&gt;

&lt;p&gt;Figure of peaks called from pseudo-bulk data:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/ch1iq5sodb6eoia/blog_tmp1.reads_v_macs3peaks.png?dl=0&quot;&gt;Number of Macs 3 peaks and Log Read Depth&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="tbr1" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_tmp1/pseudo_bulk_example Run: /home/groups/oroaklab/nishida/sc_tmp1/pseudo_bulk_example/run.txt</summary></entry></feed>