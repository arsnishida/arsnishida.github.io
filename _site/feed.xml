<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-05-11T20:03:41-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ARSN Lab Notebook</title><subtitle>ARSN Lab notebook.</subtitle><entry><title type="html">Single Cell TBR1 ENCODE DHS Index Tests</title><link href="http://localhost:4000/2021/05/11/post-0011.html" rel="alternate" type="text/html" title="Single Cell TBR1 ENCODE DHS Index Tests" /><published>2021-05-11T00:00:00-07:00</published><updated>2021-05-11T00:00:00-07:00</updated><id>http://localhost:4000/2021/05/11/post-0011</id><content type="html" xml:base="http://localhost:4000/2021/05/11/post-0011.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_tmp1/encode_DHS_index/first_pass&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run.txt&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Calling peaks on this data doesn’t work because it is too noisy. Using Casey’s cleaner peak set was working a bit better. Tiling helps sparse amounts of data but won’t help with noise. This peak set is basically Casey’s peak set+. List is from here: &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2020.06.26.172718v3&quot;&gt;biorxiv paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If the data wasn’t as noisy then calling peaks specifically across this dataset is the best approach. You’d even get novel peaks. But with technically poor data the technical methods alone do not seem to be working. Using biologically motivated references however seems to select appropriate sites for consideration in dimensionality reduction. &lt;a href=&quot;https://ohsu.app.box.com/file/809562739594&quot;&gt;figure-01&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Since this seems to be working, the dimensionality reduction needs to be re-run with filtering and doublet removal. Doublet removal on a dataset this size is giving memory issues so I am moving it to Exacloud.&lt;/p&gt;</content><author><name></name></author><category term="tbr1" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_tmp1/encode_DHS_index/first_pass</summary></entry><entry><title type="html">SPARK Mosaic List, Summary Counts of Annotations</title><link href="http://localhost:4000/2021/05/11/post-0012.html" rel="alternate" type="text/html" title="SPARK Mosaic List, Summary Counts of Annotations" /><published>2021-05-11T00:00:00-07:00</published><updated>2021-05-11T00:00:00-07:00</updated><id>http://localhost:4000/2021/05/11/post-0012</id><content type="html" xml:base="http://localhost:4000/2021/05/11/post-0012.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://www.dropbox.com/home/SPARK%20Mosaics&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This provides a full tabulated count of the annotations per category for each of the lists of SPARK mosaic data using the data without germline parental calls.&lt;/p&gt;

&lt;p&gt;Link to Markdown: &lt;a href=&quot;https://www.dropbox.com/s/bon4m7zfee6yagp/summary_fractions.html?dl=0&quot;&gt;R Markdown&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">Directory: https://www.dropbox.com/home/SPARK%20Mosaics</summary></entry><entry><title type="html">Single Cell TBR1 Coembedding Tests</title><link href="http://localhost:4000/2021/05/09/post-0010.html" rel="alternate" type="text/html" title="Single Cell TBR1 Coembedding Tests" /><published>2021-05-09T00:00:00-07:00</published><updated>2021-05-09T00:00:00-07:00</updated><id>http://localhost:4000/2021/05/09/post-0010</id><content type="html" xml:base="http://localhost:4000/2021/05/09/post-0010.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_tmp1/casey_coembed&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To address the lack of discrete clustering we used previously generated data from Casey to anchor our data.&lt;/p&gt;

&lt;p&gt;Using Casey’s peak set alone with the data creates a bit more separation. Though maybe it is mostly from how small and transparent I plotted the points on the UMAP. &lt;a href=&quot;https://ohsu.app.box.com/file/808812473361&quot;&gt;figure-01&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Direct coembedding of the data together fails. Datasets are completely separated. &lt;a href=&quot;https://ohsu.app.box.com/file/808810814006&quot;&gt;figure-02&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;- Setting up the Arrow file with coembedded data, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_1_setup.Rscript&lt;/code&gt;.
&lt;br /&gt;- Preliminary analysis with the coembedded data, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_2_coembed.Rscript&lt;/code&gt;.
&lt;br /&gt;- Commands for merging datasets and using the alternative peak set, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_coembed.txt&lt;/code&gt;.&lt;/p&gt;</content><author><name></name></author><category term="tbr1" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_tmp1/casey_coembed</summary></entry><entry><title type="html">Single Cell Alignments with no Alt Chromosomes</title><link href="http://localhost:4000/2021/05/08/post-0009.html" rel="alternate" type="text/html" title="Single Cell Alignments with no Alt Chromosomes" /><published>2021-05-08T00:00:00-07:00</published><updated>2021-05-08T00:00:00-07:00</updated><id>http://localhost:4000/2021/05/08/post-0009</id><content type="html" xml:base="http://localhost:4000/2021/05/08/post-0009.html">&lt;p&gt;Alt chromosomes in our hg38 and mm10 alignments (&lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQdownloads.html#downloadAlt&quot;&gt;UCSC description&lt;/a&gt;) cause multi-mapping in those regions. BWA flags multi-mapping reads with a MAPQ score of 0 causing them to all be lost downstream in our analysis pipeline. For most single-cell analyses we do not need to consider the alternative chromosomes and they can fall in regions of genes used as markers.&lt;/p&gt;

&lt;p&gt;Here are replaced genome builds without alts. See &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/refs/hg38/GCA_000001405.15_GRCh38/retrieval.txt&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/refs/mm10/GCA_000001635.5_GRCm38.p3_no_alt_analysis_set/retrieval.txt&lt;/code&gt; for origins of FASTA files. Indexes for BWA were recalled with v0.7.15-r1140.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FASTQ-Align Call&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ln -s /home/groups/oroaklab/demultiplex/201214_NS500556_0445_AHMF2CBGXF/201214_NS500556_0445_AHMF2CBGXF.1.fq.gz testR1.fq.gz
ln -s /home/groups/oroaklab/demultiplex/201214_NS500556_0445_AHMF2CBGXF/201214_NS500556_0445_AHMF2CBGXF.2.fq.gz testR2.fq.gz 
no_alt_align_hg38_=&quot;/home/groups/oroaklab/refs/hg38/hs38d1_noalt.fna.gz&quot;
no_alt_align_mm10_=&quot;/home/groups/oroaklab/refs/hg38/GCA_000001635.5_GRCm38.p3_noalt.fna.gz&quot;
scitools fastq-align -t 28 $no_alt_align_hg38_ testoutput testR1.fq.gz testR2.fq.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;MAPQ using standard genome with alts vs. hs38d1_noalt&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ samtools view /home/groups/oroaklab/adey_lab/projects/Panc_U54/201214_NS500556_0445_AHMF2CBGXF/201214_NS500556_0445_AHMF2CBGXF.bam chr1:198,636,928-198,659,067 | cut -f 5 | sort | uniq -c
   2500 0
$ samtools view testoutput.bam chr1:198,636,928-198,659,067 | cut -f 5 | sort | uniq -c
     19 0
      1 1
      1 6
      3 24
      1 36
      1 39
     22 40
      1 41
      1 44
      1 45
      1 49
      1 50
      1 51
      2 53
      4 54
      1 57
      2 59
   4874 60
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="scitools" /><summary type="html">Alt chromosomes in our hg38 and mm10 alignments (UCSC description) cause multi-mapping in those regions. BWA flags multi-mapping reads with a MAPQ score of 0 causing them to all be lost downstream in our analysis pipeline. For most single-cell analyses we do not need to consider the alternative chromosomes and they can fall in regions of genes used as markers.</summary></entry><entry><title type="html">Single Cell TBR1 Preliminary Evaluations</title><link href="http://localhost:4000/2021/05/07/post-0008.html" rel="alternate" type="text/html" title="Single Cell TBR1 Preliminary Evaluations" /><published>2021-05-07T00:00:00-07:00</published><updated>2021-05-07T00:00:00-07:00</updated><id>http://localhost:4000/2021/05/07/post-0008</id><content type="html" xml:base="http://localhost:4000/2021/05/07/post-0008.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_tmp1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Ryan made a first pass through this data written up on &lt;a href=&quot;https://mulqueenr.github.io/tbr1/&quot;&gt;his blog&lt;/a&gt;. I used his annotations and kept the old TSS enrichments (ArchR calculates its own internally). He made these after filtering so not all of the cells in the current ArchR project will have entries but these cells are confidently going to be filtered out. I went through ArchR up to about part 6 in the ArchR full manual. I made a broad first pass to get everything recorded, added Ryan’s existing stuff, plotted the metrics, filtered, and then did the dimensionality reduction and clustering. Then after the lab meeting I went and played around the filtering and iterative LSI options.&lt;/p&gt;

&lt;p&gt;General metrics show low TSS enrichment and higher fragment sizes. &lt;a href=&quot;https://ohsu.app.box.com/file/808812070910&quot;&gt;figure-01&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;“The bump around 200 is tagmentation on either side of a nucleosome (they are about 185bp) below that is subnucleosomal, which can be a sign of over-tagmentation or poor chromatin structure. The linker region between nucleosome is 11-80ish no but usually on the much smaller size so you don’t expect two tagmentation events between nucleosomes. I think in this case it is reflecting nucleosomes falling apart/off DNA.”
&lt;br /&gt;-Ryan&lt;/p&gt;

&lt;p&gt;Various UMAPs with different methods, variables, and cutoffs. All of the methods reveal the characteristics of a blob. &lt;a href=&quot;https://ohsu.app.box.com/file/808811822056&quot;&gt;figure-02&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;- Directory setup and bigwig creation, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run.txt&lt;/code&gt;.
&lt;br /&gt;- Setting up the Arrow file, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_1_setup.Rscript&lt;/code&gt;.
&lt;br /&gt;- Tiling matrix workaround and the main steps without filtering, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_2_process.Rscript&lt;/code&gt;.
&lt;br /&gt;- Adding annotations, plotting distributions, filtering, and dimred/clustering, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_3_filter.Rscript&lt;/code&gt;.
&lt;br /&gt;- Similar to preliminary pass but with harder filtering, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_4_harderfilter.Rscript&lt;/code&gt;.
&lt;br /&gt;- Harder filtering and alternative LSI calling, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_5_LSIfeatures_harderfilter.Rscript&lt;/code&gt;.
&lt;br /&gt;- All of the above but with also with filtering by fragment size, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_archr_6_fragFilter_LSIfeatures_harderfilter.Rscript&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;With module R/4.0.4 this little bit will let you load up the current ArchR project for yourself:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# library, resource ArchR
library(presto)
library(ArchR)

# set env
addArchRGenome(&quot;mm10&quot;)
set.seed(25)

# reload the previous project
proj &amp;lt;- loadArchRProject(path = &quot;/home/groups/oroaklab/nishida/sc_tmp1/ArchROutput/&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="tbr1" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_tmp1</summary></entry><entry><title type="html">NextSeq Fidelity Evaluation</title><link href="http://localhost:4000/2021/05/05/post-0007.html" rel="alternate" type="text/html" title="NextSeq Fidelity Evaluation" /><published>2021-05-05T00:00:00-07:00</published><updated>2021-05-05T00:00:00-07:00</updated><id>http://localhost:4000/2021/05/05/post-0007</id><content type="html" xml:base="http://localhost:4000/2021/05/05/post-0007.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/nextseq_organization_may4&lt;/code&gt;
&lt;br /&gt;Ran: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/nextseq_organization_may4/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here’s the breakdown of how our runs have fared over the years. There’s about ~300 runs post-2018 and 89 have errors that terminate NextSeq2Fastq but there’s no re-calling.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;451 Madbum Seq Runs:
	234 DO NOT have folders in both oroaklab and oroakdata
		209 are missing log files
			190 before 2018
			19 after 2018
		17 successful bcl2fastq but not attempt to backup
		7 prematurely terminate
		1 extreme outlier problem
	217 have folders in both oroaklab and oroakdata
		184 have md5sums
			176 successful copying
			8 unsuccessful copying
				6 prematurely terminate during bcl2fastq but falsely counted as successful by NextSeq2Fastq
				2 are missing log files
		27 DO NOT have md5sums
			13 successful bcl2fastq but overly cautious and terminate before copying
			3 are missing log files
			9 prematurely terminate
			2 extreme outlier problems
		6 other (alt chemistry with different formats)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Are all these prematurely terminations and missing logs bad? Yes and no. These may represent cases where people may not actually be using the data (the run is bad and isn’t used). These may also represent cases where the FASTQs are incomplete and are being used as though they were complete. But there’s no way to discern this difference, there are no records or logs meaning we can’t go back, and ultimately the data doesn’t become backed up.&lt;/p&gt;

&lt;p&gt;I fixed NextSeq2Fastq to more accurately parse the logs but this is one of the minor issues (we only have 6 instances where bcl2fastq fails but does not properly give an ERROR message so everything continues without detection). For the most part, it is accurately throwing errors in the logs and correctly stopping the process but nobody is reading the logs. If the process terminates and throws an error we need to evaluate what happened. And if you do nonstandard bcl2fastq calls or if the run has some funky chemistry you need to manually calculate md5sums and rsync the folder or we won’t have backups.&lt;/p&gt;

&lt;p&gt;Here’s an example using my adjusted NextSeq2Fastq call showing how we’re losing 75% of the data.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ perl /home/groups/oroaklab/nishida/nextseq_organization_may4/NextSeq2Fastq.pl -R 210326_NS500556_0469_AHKWVYAFX2 -f /home/groups/oroaklab/nishida/nextseq_organization_may4 -l /home/groups/oroaklab/nishida/nextseq_organization_may4/run_processing.log

f6c2b0ec6a4bcf2c6c1725bdf3fa7b47  /home/groups/oroaklab/nishida/nextseq_organization_may4/210326_NS500556_0469_AHKWVYAFX2/Undetermined_S0_R1_001.fastq.gz
0bd9dd75fbdab145d749fac73b0503db  /home/groups/oroaklab/nishida/nextseq_organization_may4/210326_NS500556_0469_AHKWVYAFX2/Undetermined_S0_R2_001.fastq.gz
86a47c890822517dc8e15f414548fedd  /home/groups/oroaklab/fastq/210326_NS500556_0469_AHKWVYAFX2/Undetermined_S0_R1_001.fastq.gz
5b5d52f587f9f6ae05292009cf39fe4f  /home/groups/oroaklab/fastq/210326_NS500556_0469_AHKWVYAFX2/Undetermined_S0_R2_001.fastq.gz

$ zcat /home/groups/oroaklab/nishida/nextseq_organization_may4/210326_NS500556_0469_AHKWVYAFX2/Undetermined_S0_R1_001.fastq.gz | wc -l
919755600
$ zcat /home/groups/oroaklab/fastq/210326_NS500556_0469_AHKWVYAFX2/Undetermined_S0_R1_001.fastq.gz  | wc -l
228860360
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;How do we fix our old runs? I think for every pre-2021 we just freeze and rsync it as it is but for everything post-2021 we redo it.&lt;/p&gt;

&lt;p&gt;Also, once the machine finally gets back on the network we are changing how Nextseq2Fastq works and nobody else will have to run it (which is why we’re looking at it now in addition to trying to backup and save space).&lt;/p&gt;</content><author><name></name></author><category term="seq" /><summary type="html">Directory: /home/groups/oroaklab/nishida/nextseq_organization_may4 Ran: /home/groups/oroaklab/nishida/nextseq_organization_may4/run.txt</summary></entry><entry><title type="html">Scitools Scrublet Function</title><link href="http://localhost:4000/2021/02/18/post-0006.html" rel="alternate" type="text/html" title="Scitools Scrublet Function" /><published>2021-02-18T00:00:00-08:00</published><updated>2021-02-18T00:00:00-08:00</updated><id>http://localhost:4000/2021/02/18/post-0006</id><content type="html" xml:base="http://localhost:4000/2021/02/18/post-0006.html">&lt;p&gt;Load the scitools python environment.
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source /home/groups/oroaklab/nishida/scitools_env/bin/activate&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Link and rename example data.
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dir=&quot;/home/groups/oroaklab/adey_lab/projects/Drosophila_Tau/201208_EL_Tau&quot;&lt;/code&gt;
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pre=&quot;201208_EL_Tau_all.no_bulk.merge.bbrd.q10.filt.macs3.TSS2plus.filt.RG.dm_peaks.w_bulk_bed.counts&quot;&lt;/code&gt;
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ln -s ${dir}/${pre}.sparseMatrix.cols.gz cols.gz&lt;/code&gt;
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ln -s ${dir}/${pre}.sparseMatrix.values.gz values.gz&lt;/code&gt;
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ln -s ${dir}/${pre}.cistopic.50.UMAP.dims umap.dims&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Run the scitools function.
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scitools matrix_scrublet values.gz cols.gz output_prefix&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Plot the values onto UMAP.
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scitools plot-dims -V output_prefix.doublet_scores.values -S 0,1 umap.dims&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Example Scrublet distribution figure: &lt;a href=&quot;https://ohsu.app.box.com/file/777868542912&quot;&gt;figure-01&lt;/a&gt;
&lt;br /&gt;Example Scrublet values projected onto UMAP figure: &lt;a href=&quot;https://ohsu.app.box.com/file/777870009592&quot;&gt;figure-02&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A bimodial scrublet distribution should make a clear case for what is and isn’t a doublet. A scrublet distribution that just tapers off is likely not flagging true doublets, just cells with weaker signal. Draw your own threshold and remove doublet-filled clusters based on the projection onto the UMAP.&lt;/p&gt;</content><author><name></name></author><category term="scitools" /><summary type="html">Load the scitools python environment. source /home/groups/oroaklab/nishida/scitools_env/bin/activate</summary></entry><entry><title type="html">Scitools scATAC Metrics</title><link href="http://localhost:4000/2020/12/17/post-0005.html" rel="alternate" type="text/html" title="Scitools scATAC Metrics" /><published>2020-12-17T00:00:00-08:00</published><updated>2020-12-17T00:00:00-08:00</updated><id>http://localhost:4000/2020/12/17/post-0005</id><content type="html" xml:base="http://localhost:4000/2020/12/17/post-0005.html">&lt;p&gt;&lt;strong&gt;TSS enrichment at a single cell level:&lt;/strong&gt;
&lt;br /&gt;Calculated as the ratio of signal to noise. Signal is defined as the region around TSS +/- 100bp. Noise is defined as the 100bp regions +/- 2000bp from TSS regions that do no overlap with signal. This leads signal and noise being slightly different in size but reads are uniquely counted into a single bin versus other methods where a read can fall into multiple TSS and noise regions. Output is a two-column value file.
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scitools bam_tssenrich [bam] [hg38 or mm10 or dm6]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TSS enrichment at bulk level, ENCODE:&lt;/strong&gt;
&lt;br /&gt;Calculated via ENCODE standards, &lt;a href=&quot;https://www.encodeproject.org/data-standards/terms/#enrichment&quot;&gt;ENCODE ENRICHMENT&lt;/a&gt;
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scitools bam_tssenrich -E [bam] [hg38 or mm10 or dm6]&lt;/code&gt;
&lt;br /&gt;Example enrichment output figure: &lt;a href=&quot;https://ohsu.app.box.com/file/754659307104&quot;&gt;figure-01&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TSS bulk calculation starts with a bam2bed conversion and is potentially intensive depending on the size of the BAM. A parallelizable version of the bulk TSS calculation can be performed on exacloud.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TSS enrichment at bulk level, inflated:&lt;/strong&gt;
&lt;br /&gt;This uses a larger distance from TSS sites, larger bins combining more reads, and chooses highest positions via, &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2020.05.10.087585v1&quot;&gt;this article&lt;/a&gt;
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scitools bam_tssenrich -E -r 2000 -n 11 -s MAX [bam] [hg38 or mm10 or dm6]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TSS sites are gathered from ENSEMBL and are curated as such:&lt;/strong&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/refs/hg38/ensembl_tss/readme.txt&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/refs/mm10/ensembl_tss/readme.txt&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/refs/dm6/ensembl_tss/readme.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Independent FRIP caller:&lt;/strong&gt;
&lt;br /&gt;This FRIP calculation is divorced from scitools atac_count and is slightly more accurate. The existing FRIP calculation that occurs during matrix generation double-counts reads that overlap multiple peaks.
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scitools bam_frip [bam] [bed]&lt;/code&gt;&lt;/p&gt;</content><author><name></name></author><category term="scitools" /><summary type="html">TSS enrichment at a single cell level: Calculated as the ratio of signal to noise. Signal is defined as the region around TSS +/- 100bp. Noise is defined as the 100bp regions +/- 2000bp from TSS regions that do no overlap with signal. This leads signal and noise being slightly different in size but reads are uniquely counted into a single bin versus other methods where a read can fall into multiple TSS and noise regions. Output is a two-column value file. scitools bam_tssenrich [bam] [hg38 or mm10 or dm6]</summary></entry><entry><title type="html">Two Week Update: 2020-11-09</title><link href="http://localhost:4000/2020/11/09/post-0004.html" rel="alternate" type="text/html" title="Two Week Update: 2020-11-09" /><published>2020-11-09T00:00:00-08:00</published><updated>2020-11-09T00:00:00-08:00</updated><id>http://localhost:4000/2020/11/09/post-0004</id><content type="html" xml:base="http://localhost:4000/2020/11/09/post-0004.html">&lt;p&gt;Mosaics - The progress report was finished and the majority of the processing and organization of the data is now complete. There are errors somewhere in the categorical classification of the putative variants and the next step is to figure out from where they are occuring. For this initial pass I have only looked at simplex quad families and now I can take another pass through the scripts and code to work on the multiple quad and trio families. As I’m going through this next pass I will make a more formal writeup and recording of the methods and where everything is located here on this blog.&lt;/p&gt;

&lt;p&gt;SPARK Single Cell / Networks - Start collapsing the original clusters into more discrete groups and get some loose pseudotime analyses done within the larger clusters. Take a look at the actual significance values of the enrichments being performed.&lt;/p&gt;

&lt;p&gt;SciTools Updates - I want to make some quick fixes to beautify the TSS enrichment plots. Also I will add hg19 into the TSS enrichment options. I have some DA updates that can get pushed as well. I also want to do another quick cleaning and updating of the dependencies. And I have handful of code related to paralellizing some operations like creating the matrix and TSS enrichment but I think I will just put them up on this blog rather than pushing them directly into the scitools code (since who knows how we will be using SLURM and exacloud in the future).&lt;/p&gt;

&lt;p&gt;Coussen’s - Get in touch with Christian to see where things were left and pass off the remaining pseudotime analyses.&lt;/p&gt;

&lt;p&gt;Misc - Write up something for the Master’s level job posting. Check in with Sally about the MIP processing. Interview with Ellen. And I’m going to mess around with the Box API because I think moving files from the cluster directly to Box or vice versa would be convenient (gets around having to both download and then upload using my own slower bandwidth).&lt;/p&gt;</content><author><name></name></author><category term="two_week" /><summary type="html">Mosaics - The progress report was finished and the majority of the processing and organization of the data is now complete. There are errors somewhere in the categorical classification of the putative variants and the next step is to figure out from where they are occuring. For this initial pass I have only looked at simplex quad families and now I can take another pass through the scripts and code to work on the multiple quad and trio families. As I’m going through this next pass I will make a more formal writeup and recording of the methods and where everything is located here on this blog.</summary></entry><entry><title type="html">Two Week Update: 2020-10-09</title><link href="http://localhost:4000/2020/10/09/post-0003.html" rel="alternate" type="text/html" title="Two Week Update: 2020-10-09" /><published>2020-10-09T00:00:00-07:00</published><updated>2020-10-09T00:00:00-07:00</updated><id>http://localhost:4000/2020/10/09/post-0003</id><content type="html" xml:base="http://localhost:4000/2020/10/09/post-0003.html">&lt;p&gt;Mosaic - Read through Deidre’s notes and now have a clear picture of the next steps in collating the data after variant calls. I will first send a draft description of what I believe the next round of summary files should look like. If these seem good I will get them all together across all the data for summarization for the progress report.&lt;/p&gt;

&lt;p&gt;Glial Atlas - Resolved the issue with global TSS enrichments and finished the dimensionality reduction on the full set of data (all peaks and all cells). Will ping Casey to get cluster information for the individual DA’s. Exacloud’s pay structure is changing so we might need to think about a longer-term solution for how to process these larger data sets.&lt;/p&gt;

&lt;p&gt;MIPs - The Friday Illumina run seemed to have died at the tail end. I demuxed everything and gave Sara her share. I need to give Sally a different suite of scripts to deal with the data quality issues of R2 for her processing.&lt;/p&gt;

&lt;p&gt;SSC - New auto-annotations were generated for each pool, manually checked, and I wrote up the few common discrepencies found. This should conclude any changes to the final variant validation tables for the time being.&lt;/p&gt;

&lt;p&gt;CRISPRcapture - The modified tables were finished and I gave some more QC metrics split out by the genome stratifications for sanity checks. Methods writeup was sent to Taylor and I’ll finish a more static summary of where everything is and put it on this blog.&lt;/p&gt;

&lt;p&gt;Spark Networks - Presented some preliminary data fixing the enrichment test and showing the enrichments across the layers/area in addition to age. Similar enrichment patterns across clusters indicate how to best group these clusters together for redoing the enrichments to give us more meaningful results (because everything is enriched currently). Do some qualitative heatmaps and pseudotime-sorted summaries of key genes.&lt;/p&gt;

&lt;p&gt;Coussen’s - Finish pseudotime DA. We will probably have to set up data migration with Google Cloud, which is what they are using instead of exacloud, eventually.&lt;/p&gt;</content><author><name></name></author><category term="two_week" /><summary type="html">Mosaic - Read through Deidre’s notes and now have a clear picture of the next steps in collating the data after variant calls. I will first send a draft description of what I believe the next round of summary files should look like. If these seem good I will get them all together across all the data for summarization for the progress report.</summary></entry></feed>