<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-07T17:02:34-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ARSN Lab Notebook</title><subtitle>ARSN Lab notebook.</subtitle><entry><title type="html">Shotgun Sequencing of Amplicon Sequence, 220305</title><link href="http://localhost:4000/2022/03/07/post-0075.html" rel="alternate" type="text/html" title="Shotgun Sequencing of Amplicon Sequence, 220305" /><published>2022-03-07T00:00:00-08:00</published><updated>2022-03-07T00:00:00-08:00</updated><id>http://localhost:4000/2022/03/07/post-0075</id><content type="html" xml:base="http://localhost:4000/2022/03/07/post-0075.html">&lt;p&gt;Dir: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/amplicon_220305&lt;/code&gt;
&lt;br /&gt;Run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/amplicon_220305/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This describes how to find our desired edit from shotgun sequencing of amplicon sequence. In lieu of standard amplicon input, we are taking the reads, aligning to hg38, and using GATK’s Haplotype Caller to identify the changes. Reads are being treated as paired-end and as a side-note about half the reads can be merged by overlap.&lt;/p&gt;

&lt;p&gt;Since this is being done relative to hg38 and not a specific amplicon sequence we have to identify what we are looking for relative to the genome. The region of interest in TBR1 is here:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# with the desired editing being designated with a '-'
# and the NHEJ region from G4 defined by '*'
&amp;gt;chr2:161,416,779-161,416,834
ref  - GTTCCCGTACCCCG*GCCAGCACGGACCGG*CGCACCCCGCCTTCTCCATCGGCAGCC
edit - GTTCCCGTACCCCG*GCCAGCACGGACCGG*CGCA-CCCGCCTTCTCCATCGGCAGCC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This makes 161,416,811 the site where ACCCC becomes ACCC which will show up as AC to A in the VCFs from Haplotype Caller.&lt;/p&gt;

&lt;p&gt;For each sample, the following output is provided:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SAMPLE - Sample ID.&lt;/li&gt;
  &lt;li&gt;EDIT_T_OR_F - True if the variant 161416811 AC to A is called. False if otherwise.&lt;/li&gt;
  &lt;li&gt;DEPTH - The depth at 161416811 if EDIT_T_OR_F is True. NA if False.&lt;/li&gt;
  &lt;li&gt;ALT_DEPTH - The depth of the edit at 161416811 if EDIT_T_OR_F is True. NA if False.&lt;/li&gt;
  &lt;li&gt;EDIT_FREQ - ALT_DEPTH / DEPTH&lt;/li&gt;
  &lt;li&gt;NUM_VARS_IN_NHEJ_REGION - The number of other variants called in the NHEJ region, 161416793-161416808.&lt;/li&gt;
  &lt;li&gt;NUM_VARS_WITHIN_10_BP - The number of other variants called +/- 10bp from 161416811 (161416801-161416821).&lt;/li&gt;
  &lt;li&gt;NUM_VARS_WITHIN_25_BP - The number of other variants called +/- 25bp from 161416811 (161416786-161416836).&lt;/li&gt;
  &lt;li&gt;NUM_VARS_WITHIN_50_BP - The number of other variants called +/- 50bp from 161416811 (161416761-161416861).&lt;/li&gt;
  &lt;li&gt;READ_DEPTH_AT_161416811 - Number of reads at the site via GATK DepthOfCoverage (not going to be the same as the depths counted via Haplotype Caller due to local alignment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This output is copy+pasted below here. This indicates the edit was found in 62 of the 156 samples. It was also found in the NTC3 sample which I am assuming is a control so there might be issues.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#SAMPLE	EDIT_T_OR_F	DEPTH	ALT_DEPTH	EDIT_FREQ	NUM_VARS_IN_NHEJ_REGION	NUM_VARS_WITHIN_10_BP	NUM_VARS_WITHIN_25_BP	NUM_VARS_WITHIN_50_BP	READ_DEPTH_AT_161416811
100	FALSE	NA	NA	NA	1	0	1	1	10296
101	FALSE	NA	NA	NA	1	0	1	1	6196
102	TRUE	2782	1874	0.673616103522646	0	0	0	0	6618
103	TRUE	2176	1299	0.596966911764706	2	0	2	2	3364
104	TRUE	2892	1530	0.529045643153527	0	0	0	1	4995
105	FALSE	NA	NA	NA	2	0	2	2	5074
106	TRUE	2697	1240	0.459770114942529	0	0	0	0	6571
107	TRUE	2793	1736	0.621553884711779	0	0	0	0	7373
108	FALSE	NA	NA	NA	1	0	1	1	5867
109	TRUE	2738	1615	0.589846603360117	0	0	0	0	6667
110	FALSE	NA	NA	NA	1	0	1	1	4203
111	FALSE	NA	NA	NA	1	0	1	1	4760
112	FALSE	NA	NA	NA	1	0	1	1	3917
113	TRUE	2827	1659	0.586841174389813	0	0	0	0	9585
114	FALSE	NA	NA	NA	0	0	0	0	1927
115	TRUE	2579	1205	0.467235362543622	0	0	0	0	5415
116	TRUE	3050	2136	0.700327868852459	0	0	0	1	8658
117	TRUE	2793	1479	0.52953813104189	0	0	0	0	7584
118	FALSE	NA	NA	NA	1	0	1	1	8982
119	FALSE	NA	NA	NA	1	0	1	1	9123
120	FALSE	NA	NA	NA	0	0	0	0	0
121	FALSE	NA	NA	NA	1	0	1	1	7319
122	TRUE	2806	1509	0.537776193870278	0	0	0	0	7244
123	TRUE	2802	1567	0.559243397573162	0	0	0	0	7008
124	TRUE	NA	NA	NA	0	0	0	0	3191
125	TRUE	2794	1603	0.573729420186113	0	0	0	0	7752
126	TRUE	2648	1584	0.598187311178248	0	0	0	0	5297
127	TRUE	2757	1878	0.681175190424374	0	0	0	0	6590
128	TRUE	2674	1637	0.612191473448018	0	0	0	0	5252
129	FALSE	NA	NA	NA	1	0	1	1	6935
130	TRUE	2614	678	0.259372609028309	0	0	0	0	5507
131	TRUE	2638	1539	0.583396512509477	1	0	1	1	8718
132	FALSE	NA	NA	NA	1	0	1	1	1
133	FALSE	NA	NA	NA	3	1	3	3	7580
134	TRUE	2566	1653	0.644193296960249	0	0	0	0	4057
135	FALSE	NA	NA	NA	1	0	1	1	8615
136	FALSE	NA	NA	NA	1	0	1	1	6476
137	TRUE	2772	2018	0.727994227994228	0	0	0	0	7440
138	FALSE	NA	NA	NA	1	0	1	1	3651
139	FALSE	NA	NA	NA	1	0	1	1	6580
140	FALSE	NA	NA	NA	2	0	2	2	60
141	FALSE	NA	NA	NA	0	0	0	0	0
142	TRUE	2541	873	0.343565525383707	2	0	2	2	6482
143	TRUE	3159	2325	0.735992402659069	0	0	0	0	7610
144	TRUE	2837	1471	0.5185054635178	0	0	0	1	5891
145	FALSE	NA	NA	NA	1	0	1	1	7478
146	FALSE	NA	NA	NA	1	0	1	1	7069
147	FALSE	NA	NA	NA	3	1	3	3	4833
148	FALSE	NA	NA	NA	1	0	1	1	4689
149	FALSE	NA	NA	NA	1	0	1	1	3455
150	TRUE	2832	1708	0.603107344632768	1	0	1	1	4757
151	FALSE	NA	NA	NA	1	0	1	1	7004
152	TRUE	NA	NA	NA	2	0	2	2	2767
153	TRUE	2832	1656	0.584745762711864	0	0	0	0	9232
154	FALSE	NA	NA	NA	1	0	1	1	8615
155	FALSE	NA	NA	NA	1	0	1	1	4943
156	FALSE	NA	NA	NA	1	0	1	1	1483
157	FALSE	NA	NA	NA	1	0	1	1	8740
158	FALSE	NA	NA	NA	1	0	1	1	4472
159	FALSE	NA	NA	NA	0	0	0	0	0
160	TRUE	143	54	0.377622377622378	0	0	0	0	93
161	TRUE	NA	NA	NA	0	0	0	0	3544
162	FALSE	NA	NA	NA	1	0	1	1	10034
163	FALSE	NA	NA	NA	1	0	1	1	8007
164	FALSE	NA	NA	NA	1	0	1	1	8149
165	FALSE	NA	NA	NA	1	0	1	1	2059
166	TRUE	2050	226	0.110243902439024	2	0	2	2	6024
167	FALSE	NA	NA	NA	1	0	1	1	8825
168	FALSE	NA	NA	NA	3	1	3	3	3551
169	TRUE	1531	1133	0.740039190071848	0	0	0	0	1461
170	FALSE	NA	NA	NA	2	0	2	2	2909
171	FALSE	NA	NA	NA	1	0	1	1	7104
172	TRUE	1775	1220	0.687323943661972	1	0	1	1	2754
173	TRUE	2795	1387	0.496243291592129	2	0	2	2	8365
174	FALSE	NA	NA	NA	1	0	1	1	4731
175	TRUE	2331	1645	0.705705705705706	1	0	1	1	8228
176	FALSE	NA	NA	NA	0	0	0	0	7518
177	TRUE	2382	1397	0.586481947942905	0	0	0	0	4829
178	FALSE	NA	NA	NA	3	1	3	3	3881
179	TRUE	2827	1491	0.527414220021224	0	0	0	0	8273
180	TRUE	2283	282	0.123521681997372	2	0	2	2	6998
181	FALSE	NA	NA	NA	1	0	1	1	5274
182	FALSE	NA	NA	NA	0	0	0	0	132
183	FALSE	NA	NA	NA	0	0	0	0	88
184	FALSE	NA	NA	NA	2	0	2	2	6233
185	TRUE	2740	954	0.348175182481752	0	0	0	0	7204
186	FALSE	NA	NA	NA	3	1	3	3	3485
187	FALSE	NA	NA	NA	3	1	3	3	5168
188	TRUE	2789	1680	0.60236643958408	0	0	0	0	8810
189	FALSE	NA	NA	NA	2	1	3	3	7601
190	TRUE	2770	1409	0.508664259927798	0	0	0	0	7554
191	FALSE	NA	NA	NA	1	0	1	1	5427
192	TRUE	2149	223	0.103769194974407	0	0	0	0	2857
193	FALSE	NA	NA	NA	1	0	1	1	596
194	FALSE	NA	NA	NA	1	0	1	1	39
195	TRUE	212	120	0.566037735849057	0	0	0	0	153
196	TRUE	7	2	0.285714285714286	0	0	0	0	7
197	FALSE	NA	NA	NA	1	0	1	1	32
198	TRUE	89	45	0.50561797752809	0	0	0	0	75
199	FALSE	NA	NA	NA	1	0	1	1	250
200	FALSE	NA	NA	NA	0	0	0	0	117
201	FALSE	NA	NA	NA	1	0	1	1	39
202	FALSE	NA	NA	NA	3	1	3	3	324
203	FALSE	NA	NA	NA	0	0	0	0	0
204	TRUE	13	4	0.307692307692308	0	0	0	0	11
205	TRUE	477	352	0.737945492662474	0	0	0	0	314
206	FALSE	NA	NA	NA	1	0	1	1	398
207	FALSE	NA	NA	NA	1	0	1	1	258
208	TRUE	212	58	0.273584905660377	0	0	0	0	156
209	FALSE	NA	NA	NA	1	0	1	1	12
210	FALSE	NA	NA	NA	1	0	1	1	383
211	FALSE	NA	NA	NA	0	0	0	0	1
212	FALSE	NA	NA	NA	1	0	1	1	130
213	TRUE	17	7	0.411764705882353	2	1	2	3	16
214	FALSE	NA	NA	NA	1	0	1	1	494
215	FALSE	NA	NA	NA	0	0	0	0	107
216	FALSE	NA	NA	NA	1	0	1	1	294
217	FALSE	NA	NA	NA	1	0	1	1	105
218	TRUE	NA	NA	NA	2	0	2	2	39
219	FALSE	NA	NA	NA	1	0	1	1	150
220	TRUE	583	417	0.715265866209262	0	0	0	0	474
221	FALSE	NA	NA	NA	1	0	1	1	200
222	TRUE	452	222	0.491150442477876	0	0	0	0	400
223	FALSE	NA	NA	NA	0	0	0	0	394
224	FALSE	NA	NA	NA	0	0	0	0	162
225	TRUE	441	43	0.0975056689342404	0	0	0	0	347
226	TRUE	675	455	0.674074074074074	0	0	0	0	571
227	TRUE	55	16	0.290909090909091	0	0	0	0	38
228	FALSE	NA	NA	NA	0	0	0	0	0
229	FALSE	NA	NA	NA	3	1	3	3	19
230	FALSE	NA	NA	NA	1	0	1	1	6
231	TRUE	311	169	0.543408360128617	2	0	2	2	226
232	FALSE	NA	NA	NA	1	0	1	1	220
233	FALSE	NA	NA	NA	1	0	1	1	194
234	TRUE	116	99	0.853448275862069	0	0	0	0	86
235	FALSE	NA	NA	NA	0	0	0	0	0
236	FALSE	NA	NA	NA	0	0	0	0	286
237	FALSE	NA	NA	NA	1	0	1	1	281
238	FALSE	NA	NA	NA	2	0	2	2	425
239	FALSE	NA	NA	NA	0	0	0	0	0
240	TRUE	357	232	0.649859943977591	0	0	0	0	266
241	FALSE	NA	NA	NA	1	0	1	1	123
242	FALSE	NA	NA	NA	1	0	1	1	543
243	TRUE	186	31	0.166666666666667	0	0	0	1	169
244	FALSE	NA	NA	NA	2	0	2	2	739
245	FALSE	NA	NA	NA	1	0	1	1	256
246	FALSE	NA	NA	NA	3	1	3	3	289
247	TRUE	245	185	0.755102040816326	0	0	0	0	163
97	FALSE	NA	NA	NA	1	0	1	1	7129
98	FALSE	NA	NA	NA	1	0	1	1	4407
99	TRUE	2542	2016	0.793076317859953	1	0	1	1	7855
A136P	TRUE	1003	655	0.653040877367896	0	0	0	0	890
CIRM87	FALSE	NA	NA	NA	0	0	0	0	850
NTC1	FALSE	NA	NA	NA	0	0	0	0	0
NTC2	FALSE	NA	NA	NA	0	0	0	0	1
NTC3	TRUE	94	84	0.893617021276596	0	0	0	0	85
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="amplicon" /><summary type="html">Dir: /home/groups/oroaklab/nishida/amplicon_220305 Run: /home/groups/oroaklab/nishida/amplicon_220305/run.txt</summary></entry><entry><title type="html">OneDrive Upload and Download with Rclone</title><link href="http://localhost:4000/2022/03/01/post-0074.html" rel="alternate" type="text/html" title="OneDrive Upload and Download with Rclone" /><published>2022-03-01T00:00:00-08:00</published><updated>2022-03-01T00:00:00-08:00</updated><id>http://localhost:4000/2022/03/01/post-0074</id><content type="html" xml:base="http://localhost:4000/2022/03/01/post-0074.html">&lt;p&gt;Here’s instructions for using rclone on our clusters specifically using a remote/headless machine. This requires more setup but skips the use of X11.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download rclone on your personal home computer, https://rclone.org/downloads/.&lt;/li&gt;
  &lt;li&gt;Go to mays/mccovey/bonds and load the rclone modules, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module load rclone&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Setup rclone on mays/mccovey/bonds, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rclone config&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Inside rclone’s configuration, setup the following:
    &lt;ul&gt;
      &lt;li&gt;‘n’ for new remote at the ‘n/s/q’ prompt&lt;/li&gt;
      &lt;li&gt;a shortcut name for the storage at the ‘name’ prompt, I am just calling it ONEDRIVE&lt;/li&gt;
      &lt;li&gt;‘onedrive’ for the ‘Storage’ prompt&lt;/li&gt;
      &lt;li&gt;press enter to skip the prompt ‘client_id’&lt;/li&gt;
      &lt;li&gt;press enter to skip the prompt ‘client_secret’&lt;/li&gt;
      &lt;li&gt;‘global’ for the prompt ‘region’&lt;/li&gt;
      &lt;li&gt;‘n’ to not edit the advanced configurations at ‘y/n’&lt;/li&gt;
      &lt;li&gt;‘n’ to setup on the headless node at ‘y/n’&lt;/li&gt;
      &lt;li&gt;leave this open for a moment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;On your personal home computer run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rclone authorize &quot;onedrive&quot;&lt;/code&gt;. This will open your web browser and ask you to log into your OneDrive account. This then returns a long configuration string to copy.&lt;/li&gt;
  &lt;li&gt;Paste the configuration key back on the config in mays/mccovey/bonds.&lt;/li&gt;
  &lt;li&gt;Finish configuration:
    &lt;ul&gt;
      &lt;li&gt;confirm the configuration as ‘onedrive’ at the prompt ‘config_type’&lt;/li&gt;
      &lt;li&gt;confirm ‘y’ at the URL confirmation of ‘y/n’&lt;/li&gt;
      &lt;li&gt;confirm ‘y’ for setup at the last prompt ‘y/n’&lt;/li&gt;
      &lt;li&gt;‘q’ to quit out and you should be set up&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Send a test file. This should send the test file to your OneDrive account setup with the name “ONEDRIVE” to the directory Documents/
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo &quot;THIS IS A TEST FILE&quot; &amp;gt; test.txt
rclone copy test.txt ONEDRIVE:Documents/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="ref" /><summary type="html">Here’s instructions for using rclone on our clusters specifically using a remote/headless machine. This requires more setup but skips the use of X11.</summary></entry><entry><title type="html">Dropbox Upload and Download with Curl and API Key</title><link href="http://localhost:4000/2022/01/25/post-0073.html" rel="alternate" type="text/html" title="Dropbox Upload and Download with Curl and API Key" /><published>2022-01-25T00:00:00-08:00</published><updated>2022-01-25T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/25/post-0073</id><content type="html" xml:base="http://localhost:4000/2022/01/25/post-0073.html">&lt;p&gt;These are the step to upload and download from the console to and from Dropbox.
&lt;br /&gt;1. Go to: https://www.dropbox.com/developers/apps
&lt;br /&gt;2. Create an App. Name is whatever you like but probably something like “Dropbox Access”.
&lt;br /&gt;3. Navigate to Permissions on the top tab. Check account_info.write, account_info.read, files.metadata.write, files.metadata.read, files.content.write, and files.content.read.
&lt;br /&gt;4. Navigate to Settings on the top tab. Generate your access token. If you change your permissions you must regenerate your access token. Always keep your API key secret or else anybody can use it to access everything in your Dropbox.&lt;/p&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;&lt;b&gt;For files &amp;lt; 150 Mb using the “2/files/upload” endpoint.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Upload Instructions. This will upload file, ‘test.txt’, into your Dropbox folder ‘temp_upload’ under the name ‘uploaded_test_file.txt’.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# create test file
echo &quot;TEST THIS&quot; &amp;gt; test.txt

# fill these variables in
API_KEY=&quot;keep your API key secret&quot;
INPUT_PATH=&quot;test.txt&quot;
DROPBOX_FILE_PATH=&quot;/temp_upload/uploaded_test_file.txt&quot;

# run
curl -X POST https://content.dropboxapi.com/2/files/upload \
    --header &quot;Authorization: Bearer ${API_KEY}&quot; \
    --header &quot;Dropbox-API-Arg: {\&quot;path\&quot;: \&quot;${DROPBOX_FILE_PATH}\&quot;}&quot; \
    --header &quot;Content-Type: application/octet-stream&quot; \
    --data-binary @${INPUT_PATH}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Download Instructions. This will download ‘uploaded_test_file.txt’ in Dropbox directory ‘temp_upload’ as ‘downloaded_test_file.txt&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# fill these variables in
API_KEY=&quot;keep your API key secret&quot;
DROPBOX_FILE_PATH=&quot;/temp_upload/uploaded_test_file.txt&quot;
OUTPUT_PATH=&quot;downloaded_test_file.txt&quot;

# run
curl -X POST https://content.dropboxapi.com/2/files/download \
    --header &quot;Authorization: Bearer ${API_KEY}&quot; \
    --header &quot;Dropbox-API-Arg: {\&quot;path\&quot;: \&quot;${DROPBOX_FILE_PATH}\&quot;}&quot; \
    -o &quot;${OUTPUT_PATH}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;p&gt;&lt;b&gt;For files &amp;gt; 150 Mb using upload sessions.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;https://github.com/andreafabrizi/Dropbox-Uploader&lt;/p&gt;</content><author><name></name></author><category term="ref" /><summary type="html">These are the step to upload and download from the console to and from Dropbox. 1. Go to: https://www.dropbox.com/developers/apps 2. Create an App. Name is whatever you like but probably something like “Dropbox Access”. 3. Navigate to Permissions on the top tab. Check account_info.write, account_info.read, files.metadata.write, files.metadata.read, files.content.write, and files.content.read. 4. Navigate to Settings on the top tab. Generate your access token. If you change your permissions you must regenerate your access token. Always keep your API key secret or else anybody can use it to access everything in your Dropbox.</summary></entry><entry><title type="html">Drosophilia Tau DA Results Test Analysis</title><link href="http://localhost:4000/2022/01/21/post-0072.html" rel="alternate" type="text/html" title="Drosophilia Tau DA Results Test Analysis" /><published>2022-01-21T00:00:00-08:00</published><updated>2022-01-21T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/21/post-0072</id><content type="html" xml:base="http://localhost:4000/2022/01/21/post-0072.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis&lt;/code&gt;
&lt;br /&gt;Run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This is an example rough pass through analyzing the results of the DA test. This focuses on the ‘3_Parent_d5’ comparisons as the example. Clusters are combined across treatments but kept separate. Thresholds for ‘significantly changing’ sites are &amp;lt; 0.01 raw p-value and &amp;gt; 1.5 Log2 FC. The significant results could be annotated by their motifs and nearest genes.&lt;/p&gt;

&lt;p&gt;5.2-5.7 - P-value distributions are left-skewed. FDR does not work except maybe with WT_to_KI_KI. Volcano plots suggest global opening and closing in some conditions (WT_to_KI_KI shows a global decrease in accessibility across many sites).
8.8 - Heatmap shows groups of sites that are specific to the strains trending in the same direction. The most notable is the group for WT to KI_KI which shows large decreases specific to the biological cluster 3 (in yellow on the left).&lt;/p&gt;

&lt;p&gt;Markdown links:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/2vbwjy5j27zohi0/dm6_test_da.html?dl=0&quot;&gt;R Markdown HTML&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/an2v1ngucr8nqlt/dm6_test_da.Rmd?dl=0&quot;&gt;R Markdown&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="dm6_tau" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis Run: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/test_analysis/run.txt</summary></entry><entry><title type="html">SPARK Pre Best Practices Filter, Model Scores</title><link href="http://localhost:4000/2022/01/19/post-0071.html" rel="alternate" type="text/html" title="SPARK Pre Best Practices Filter, Model Scores" /><published>2022-01-19T00:00:00-08:00</published><updated>2022-01-19T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/19/post-0071</id><content type="html" xml:base="http://localhost:4000/2022/01/19/post-0071.html">&lt;p&gt;This markdown takes the pre-filtered data and plots out the metrics that are used for scoring by the logistic model. All variants are plotted and then just the subset of mosaic child calls.&lt;/p&gt;

&lt;p&gt;This data represents the simplex quads only. It uses the broad filter and removes parental germline calls.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A broad filter to reduce computational load is used.
- Frequent variants appearing 5 or more times are removed from further analysis.
- Common variant allele frequencies determined from ExAC and gnomAD annotations must be less than 0.005.
- Variants require at least 3 non-reference reads and at least 8 reads in all four family members.
- Variants must also fall within exonic or splicing regions from RefSeq annotations.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Markdown showing the error model metrics of the pre-filtered data:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/md88g4ahmmwuer9/20_logistic_model_params.html?dl=0&quot;&gt;20 Logistic Model Metrics Pre-Filter&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">This markdown takes the pre-filtered data and plots out the metrics that are used for scoring by the logistic model. All variants are plotted and then just the subset of mosaic child calls.</summary></entry><entry><title type="html">SPARK SSC hg38 Transformed Burden, 01</title><link href="http://localhost:4000/2022/01/18/post-0070.html" rel="alternate" type="text/html" title="SPARK SSC hg38 Transformed Burden, 01" /><published>2022-01-18T00:00:00-08:00</published><updated>2022-01-18T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/18/post-0070</id><content type="html" xml:base="http://localhost:4000/2022/01/18/post-0070.html">&lt;p&gt;The SSC list was transformed from hg19 to hg38 as described &lt;a href=&quot;https://arsnishida.github.io/2021/11/30/post-0066.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;20 sites were removed for having no original FRACTION assignment. 172 sites removed for falling into a SDTRF region in going from hg19 to hg38. 9 sites were removed for having no functional mutation type (both originally and after reannotation). There were 7 mosaic child sites lost in this fashion. 1 had no mutation type in both hg19 and hg38. The other 6 fell in super duplicate regions with 3 of them being within 100 bp of each other constituting a complex event.&lt;/p&gt;

&lt;p&gt;This recreates the burden analysis from the original SSC paper. The trends match but the n’s and signficance do not. This is due to the family list and potentially the LGD/NS lists. The family lists include hundreds more families than the original analysis and are taken from ‘cohort_all_families_list.table’. The LGD/NS lists were taken from ‘Family_Subcohort_Designations.xlsx’. These were found in our Box directory:
&lt;br /&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/All Files/kruppd/Writing/SSC somatic pilot/data/Data Used for Mosaic Paper/High Confidence Variants/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Markdown showing the SSC solo burden analysis:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/0pv9gk7cxk24v9j/19_burden_genesets_ssc_missense.html?dl=0&quot;&gt;19 SSC Solo Burden Markdown&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">The SSC list was transformed from hg19 to hg38 as described here.</summary></entry><entry><title type="html">Drosophilia Tau Final DA</title><link href="http://localhost:4000/2022/01/08/post-0069.html" rel="alternate" type="text/html" title="Drosophilia Tau Final DA" /><published>2022-01-08T00:00:00-08:00</published><updated>2022-01-08T00:00:00-08:00</updated><id>http://localhost:4000/2022/01/08/post-0069</id><content type="html" xml:base="http://localhost:4000/2022/01/08/post-0069.html">&lt;p&gt;Directory: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022&lt;/code&gt;
&lt;br /&gt;Run: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/run.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The DA was performed after TF normalization using the Wilcoxon rank sum test and creates the comparisons in ‘210712_DA_Comparisons.txt’. Mild name changes were made in the comparisons such as ‘Photoreceptor’ to ‘Photoreceptors’ to match the annotation file and using the correct annotation column for the biologically split subsets.&lt;/p&gt;

&lt;p&gt;The following comparisons are nonexistant because they have no cells with the annotation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Glia_WT_CS_5_C13 has no cells so all 6 comparisons are missing in 3_Glia_d5.
- Glia_KI_CS_30_C21, Glia_dTaudel_30_C13, Glia_KI_CS_30_C13 in 3_Glia_d30.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following comparisons are deliberately empty because they have too few cells:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- DA.Glia_WT_CS_30_C11_Glia_PL_CS_30_C11.txt, DA.Glia_WT_CS_30_C13_Glia_CS_30_C13.txt
- DA.Neuron_WT_CS_30_C14_Neuron_dTaudel_30_C14.txt, DA.Neuron_WT_CS_30_C14_Neuron_PL_CS_30_C14.txt
- DA.Photoreceptors_WT_CS_30_C1_Photoreceptors_PL_CS_30_C1.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_dTaudel_30_C3.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_KI_CS_30_C3.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_PL_CS_30_C3.txt, DA.Photoreceptors_WT_CS_30_C3_Photoreceptors_VM_CS_30_C3.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="dm6_tau" /><summary type="html">Directory: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022 Run: /home/groups/oroaklab/nishida/sc_dm6_tau/DA_jan7_2022/run.txt</summary></entry><entry><title type="html">SPARK VEP Annotations</title><link href="http://localhost:4000/2021/12/03/post-0068.html" rel="alternate" type="text/html" title="SPARK VEP Annotations" /><published>2021-12-03T00:00:00-08:00</published><updated>2021-12-03T00:00:00-08:00</updated><id>http://localhost:4000/2021/12/03/post-0068</id><content type="html" xml:base="http://localhost:4000/2021/12/03/post-0068.html">&lt;p&gt;VEP was used to annotate the high confidence lists. The following databases were used. There are redundant scores between the databases but they might possibly differ slightly due to the different versions of references each uses.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;dbNSFP v4.1a from ftp://dbnsfp:dbnsfp@dbnsfp.softgenetics.com/dbNSFP4.1a.zip&lt;/li&gt;
  &lt;li&gt;gnomAD v3.0.1 from https://storage.googleapis.com/gcp-public-data–gnomad/release/3.0.1/coverage/genomes/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz&lt;/li&gt;
  &lt;li&gt;REVEL v1.3 from https://sites.google.com/site/revelgenomics/downloads&lt;/li&gt;
  &lt;li&gt;LoFtool scores (no version) from https://github.com/Ensembl/VEP_plugins&lt;/li&gt;
  &lt;li&gt;ExACpLI scores (no version) from https://github.com/Ensembl/VEP_plugins&lt;/li&gt;
  &lt;li&gt;blosum62 scores (no version)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am keeping this separated from the high confidence lists but these columns can be directly added to the highconfidence lists without any reordering (the rows are the same). These are available here. 
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/bjwj52ibth2rytw/quadsimplex.highconfidence.add_annot.txt?dl=0&quot;&gt;Simplex Quad, Added Annotations&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/fgr8kt3mt98juq5/quadmultiplex.highconfidence.add_annot.txt?dl=0&quot;&gt;Multiplex Quad, Added Annotations&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/ntn4cr1jmh44q70/triosimplex.highconfidence.add_annot.txt?dl=0&quot;&gt;Simplex Trio, Added Annotations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The dbNSFP columns are described here and then are followed by the other scores from the other annotation databases:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/j95lkv05al56kcp/dbNSFP_columns.txt?dl=0&quot;&gt;Added Annotation Column Descriptions&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">VEP was used to annotate the high confidence lists. The following databases were used. There are redundant scores between the databases but they might possibly differ slightly due to the different versions of references each uses. dbNSFP v4.1a from ftp://dbnsfp:dbnsfp@dbnsfp.softgenetics.com/dbNSFP4.1a.zip gnomAD v3.0.1 from https://storage.googleapis.com/gcp-public-data–gnomad/release/3.0.1/coverage/genomes/gnomad.genomes.r3.0.1.coverage.summary.tsv.bgz REVEL v1.3 from https://sites.google.com/site/revelgenomics/downloads LoFtool scores (no version) from https://github.com/Ensembl/VEP_plugins ExACpLI scores (no version) from https://github.com/Ensembl/VEP_plugins blosum62 scores (no version)</summary></entry><entry><title type="html">SPARK Burden Tables Gene Sets, SPARK+SSC</title><link href="http://localhost:4000/2021/11/30/post-0067.html" rel="alternate" type="text/html" title="SPARK Burden Tables Gene Sets, SPARK+SSC" /><published>2021-11-30T00:00:00-08:00</published><updated>2021-11-30T00:00:00-08:00</updated><id>http://localhost:4000/2021/11/30/post-0067</id><content type="html" xml:base="http://localhost:4000/2021/11/30/post-0067.html">&lt;p&gt;This performs the burden analysis across the genesets using the combined and harmonized Spark and SSC lists. The SSC is transformed to hg38 but the joint coverages remain in hg19. Joint coverages were manually filtered for duplicate entries for two families, 13431 and 12839.&lt;/p&gt;

&lt;p&gt;Markdowns:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/9j19yvd2bc66lyi/18_burden_genesets_qsts_ssc_missense.html?dl=0&quot;&gt;18 Burden Genesets, SPARK+SSC and missense&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/nh5cqxu9qosk6aj/18_burden_genesets_qsts_ssc_synonymous.html?dl=0&quot;&gt;18 Burden Genesets, SPARK+SSC and synonymous&lt;/a&gt;
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/lu5b5ruetw526jr/18_burden_genesets_qsts_ssc_nonsensesplicing.html?dl=0&quot;&gt;18 Burden Genesets, SPARK+SSC and nonsense-splicing&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">This performs the burden analysis across the genesets using the combined and harmonized Spark and SSC lists. The SSC is transformed to hg38 but the joint coverages remain in hg19. Joint coverages were manually filtered for duplicate entries for two families, 13431 and 12839.</summary></entry><entry><title type="html">SPARK SSC List Formatting to Current SPARK</title><link href="http://localhost:4000/2021/11/30/post-0066.html" rel="alternate" type="text/html" title="SPARK SSC List Formatting to Current SPARK" /><published>2021-11-30T00:00:00-08:00</published><updated>2021-11-30T00:00:00-08:00</updated><id>http://localhost:4000/2021/11/30/post-0066</id><content type="html" xml:base="http://localhost:4000/2021/11/30/post-0066.html">&lt;p&gt;The SSC variant calls are taken from this Box file, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://ohsu.box.com/s/dqm3garunt0xobvgdz41onpexi87yqin&lt;/code&gt;, on this path, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;All Files/kruppd/Writing/SSC somatic pilot/data/Data Used for Mosaic Paper/High Confidence Variants/HighConfidence Variants Collapsed_2.csv&lt;/code&gt;. This appears to be a similar version of Table S5 from the Krupp 2017 paper with the columns with the statistics which are excluded from the paper. Unlike the table in the paper, this also collapses sites in overlapping genes into a single entry which is more in line with the SPARK lists where genes are combined in one entry separated by commas. All SSC columns are transformed to match the columns for the SPARK list.&lt;/p&gt;

&lt;p&gt;Here is the transformed SSC list. It has the same columns as the SPARK lists and has extra columns representing the SSC specific entries. The two important extra columns to consider that are unique to the SSC list are column 90 and 91 which are the ‘FILTER’ and ‘OLD_TYPE’ respectively. ‘FILTER’ is TRUE or FALSE. TRUE entries should be filtered from the analysis and their designation is explained below. In general they are calls that do not meet our criteria after reannotation in hg38 versus hg19. ‘OLD_TYPE’ designates whether the call comes from a quad or trio family which may or may not be important for us.&lt;/p&gt;

&lt;p&gt;SSC list in SPARK list format:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/hkjkeqibfk2nkyw/ssc.transformed.txt?dl=0&quot;&gt;SSC List in SPARK List Format&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Markdown showing how the list was modified:
&lt;br /&gt;&lt;a href=&quot;https://www.dropbox.com/s/j1flcekwsrp6s03/17_ssc_list.html?dl=0&quot;&gt;17 SSC List Formatting to Current SPARK&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The column ‘FILTER’ designates calls to remove from downstream analysis. ‘TRUE’ calls should be removed and are given if:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- there are no FRACTION calls made (not labeled GERMLINE or MOSAIC, labeled 'NA')
- the variant falls in a region that is annotated in either a non-exonic, super duplicate, or trf region
- the variant does not have a predicted functional mutation type
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following columns were modified in the transformation:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Coordinates were changed via LiftOver with 100% of the sites found going from hg19 to hg38.
All genomic and functional annotations were redone with AnnoVAR for hg38.
FRACTION_MORE is derived from FRACTION with name changes to match the SPARK list (&quot;GERMLINE_C&quot; is changed to &quot;GERMLINE_CHILD&quot;).
ACC_DIST and DON_DIST are the same value. The SSC list keeps only the closest wheras the SPARK list keeps both separate.
COMPLEX_EVENT is descriptive in the SSC list and binary in the SPARK list.
NUM_FAMILY_FOUND and filteredCohortCount represent cohort counts before and after filtering. In the SSC list, only the final count is kept so these are the same for the SSC list.
HGNCsymbol_to_ENSID and RefSeq_to_ENSID are the ENSEMBL ID's determined from the hg38 annotations.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following columns were dropped and all set to ‘NA’:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;There are no numerical sample IDs in the SSC list.
SSC uses combined family stats from the variant callers whereas the SPARK list keeps them by individual so these are dropped.
Mismatch stats for raw mismatch counts, 'other' read counts, and the means are not present in the SSC list.
gnomAD allele fractions are not present in the earlier SSC table.
The only error columns in the SSC list are the calculated rates.
FRACTION_OLD is a legacy column in the SPARK list and does not have meaning in the SSC list.
The splice site transcript IDs are not present just the distances.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following extra columns specific to the SSC list were added at the end. These are potentially important for analysis:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FILTER - TRUE or FALSE and represent variants that should be filtered out after the transformation of hg19 to hg38
OLD_TYPE - TRIO or QUAD
OLD_SEGDUP, OLD_RMSK, OLD_TRF, OLD_FUNC_TYPE, OLD_TRANS_REF, OLD_CLASS_REF, OLD_GENE_REF - these are the original hg19 AnnoVAR annotations
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following SSC specific columns were kept and added at the end. These can likely be ignored. I’ve tried to describe them as best as possible without knowing their exact provenance:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;OLD_STUDY - COHORT or PILOT24 or PILOT400
OLD_PUBLISHED - IOSSIFOV_2014 or KRUMM_2015
OLD_AF_ESP, OLD_AF_1KG - allele fractions from other databases
OLD_PCTILE_CDS_AUTO_45X - 5% percentiles of CDS AUTO 45x
OLD_FORMAT_VARSCAN, OLD_FORMAT_LOFREQ, OLD_FORMAT_SMOC, OLD_VARSCAN_232_FA, OLD_LOFREQ_211_FA, OLD_SMOC_FA, OLD_VARSCAN_232_MO, OLD_LOFREQ_211_MO, OLD_SMOC_MO, OLD_VARSCAN_232_PRO, OLD_LOFREQ_211_PRO, OLD_SMOC_PRO, OLD_VARSCAN_232_SIB, OLD_LOFREQ_211_SIB, OLD_SMOC_SIB - family-based variant calling output
OLD_DIST_TO_PREV, OLD_DIST_TO_NEXT, OLD_PERSON_PREV, OLD_PERSON_NEXT - closest call and person	
OLD_QC_STATUS - at this stage they are all 'PASS', unknown criteria
OLD_COHORT_DP, OLD_COHORT_DPALT, OLD_P400_COUNT_MOS, OLD_P400_COUNT_GERM, OLD_COHORT_COUNT_MOS, OLD_COHORT_COUNT_GERM, OLD_COHORT_MAX_AD, OLD_P400_COUNT_ALL - various counts within the cohort subsets
OLD_AF_FA, OLD_AF_MO, OLD_AF_PRO, OLD_AF_SIB, OLD_CALLED_FA, OLD_CALLED_MO, OLD_CALLED_PRO, OLD_CALLED_SIB, OLD_DETECT_FA, OLD_DETECT_MO, OLD_DETECT_PRO, OLD_DETECT_SIB, OLD_PHET_MAIN, OLD_SITE_KEY, OLD_PARENTAL, OLD_GONADAL, OLD_CALLED_VARSCAN, OLD_CALLED_LOFREQ, OLD_CALLED_SMOC - these are all redundant derived columns
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="SPARK" /><summary type="html">The SSC variant calls are taken from this Box file, https://ohsu.box.com/s/dqm3garunt0xobvgdz41onpexi87yqin, on this path, All Files/kruppd/Writing/SSC somatic pilot/data/Data Used for Mosaic Paper/High Confidence Variants/HighConfidence Variants Collapsed_2.csv. This appears to be a similar version of Table S5 from the Krupp 2017 paper with the columns with the statistics which are excluded from the paper. Unlike the table in the paper, this also collapses sites in overlapping genes into a single entry which is more in line with the SPARK lists where genes are combined in one entry separated by commas. All SSC columns are transformed to match the columns for the SPARK list.</summary></entry></feed>